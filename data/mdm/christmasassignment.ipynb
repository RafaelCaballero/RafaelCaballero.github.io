{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Christmas Assignment\n",
    "\n",
    "### MDM - Rafael Caballero\n",
    "\n",
    "Due to a computer virus, the extesions of several files have been mixed.\n",
    "Fortunately, we had collected data of some of the files in advance, in particular:\n",
    "\n",
    "     |-- length: integer (nullable = true)  length of the name of the file without extension\n",
    "     |-- kbytes: integer (nullable = true)  Size of the file in Kbytes \n",
    "     |-- curlybrackets: integer (nullable = true) Number of characters {,} in the file\n",
    "     |-- roundbrackets: double (nullable = true) Number of characters (,) in the file\n",
    "     |-- colon: double (nullable = true) Number of :\n",
    "     |-- semicolon: double (nullable = true) Number of ;\n",
    "     |-- comma: double (nullable = true) Number of ,\n",
    "     |-- other: double (nullable = true) Other characters\n",
    "     |-- lines: double (nullable = true) Number of lines\n",
    "     |-- words: double (nullable = true) Number of words\n",
    "     |-- lower: double (nullable = true) Number of lowercase characters\n",
    "     |-- upper: double (nullable = true) Number of uppeercase characters\n",
    "     |-- digit: double (nullable = true) Number of digits\n",
    "     |-- relat: double (nullable = true) Number of relational operators <,>,=...\n",
    "     |-- space: double (nullable = true) Number of blank characters\n",
    "     |-- ext: string (nullable = true)   File extension, can be either \"ipynb\", or \"java\"\n",
    " \n",
    " Our goal is to use Machine Learning in Spark to predict the extension (ext) from the rest of columns. The code is free but following these rules\n",
    "\n",
    "1.- Use any Spark model, transformer, etc. You can also try models or transformers that we haven't seen in class. Explore new possibilities!\n",
    " \n",
    "2.- The code must include a variable `model` With the final model. Use a *pipeline* (see the (Pipeline Notebook[http://gpd.sip.ucm.es/rafa/docencia/mtda/mdm/code/pipelines_en.ipynb]) in order to combina in the model all the preprocessing steps.\n",
    "\n",
    "3.- The data does not include null. Although you can (and should) make many experiments searching for the best model, please only include the code of the last and definitive `model`\n",
    " \n",
    "4.- The model generation process cannot take more that 10 seconds in the virtual machine. If you use any hyperparameter of integer type, the maximum value it can take is 100.\n",
    "\n",
    "5.- The evaluation of the assignment will be obtained using the \"Test Section\". I will use different data for the evaluation, but this is not important.\n",
    "\n",
    "6.- The score will depend on the kappa metric returned by the function `evaluate_predictions`. To pass the test a minimum of k=0.58 is required.\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from subprocess import check_call\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "import findspark\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "modules = [\"findspark\",\"pyspark\"]\n",
    "\n",
    "def check_modules(modules, upgrade=False):\n",
    "    print(\"Checking required modules\")\n",
    "    for m in modules:\n",
    "        torch_loader = importlib.util.find_spec(m)\n",
    "        if torch_loader is not None and not upgrade:\n",
    "            print(m,\" found\")\n",
    "        else:\n",
    "            if upgrade:\n",
    "                print(\"upgrading \",m)\n",
    "            else:\n",
    "                print(m,\" not found, installing\")\n",
    "            if 'google.colab' in sys.modules:\n",
    "                if upgrade:\n",
    "                    check_call([\"pip\", \"install\", \"--upgrade\", m])\n",
    "                else:\n",
    "                    check_call([\"pip\", \"install\", \"-q\", m])\n",
    "            else:\n",
    "                if upgrade:\n",
    "                    check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"--upgrade\", m])\n",
    "                else:\n",
    "                    check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", m])\n",
    "\n",
    "check_modules(modules,upgrade=False)\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.getOrCreate() # SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "df = spark.sql('''select 'spark' as hi ''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next code includes a few useful functions, in particular `evaluate_predictions` which we will use to obtain the kappa metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = \"./twords.csv\"\n",
    "url = \"https://raw.githubusercontent.com/RafaelCaballero/tdm/master/datos/filetrain.csv\"\n",
    "file = \"./filetrain.csv\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pyspark_dist_explore import hist\n",
    "import pyspark.sql.functions as func \n",
    "\n",
    "\n",
    "def load_file(file):\n",
    "    df = spark.read.format(\"com.databricks.spark.csv\")\\\n",
    "            .options(header='true', inferschema='true') \\\n",
    "            .load(file)\n",
    "    return df\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_predictions(predictions,verb=True):\n",
    "    \n",
    "    preds_and_labels = predictions.select(['prediction','label'])\n",
    "    pl = preds_and_labels.rdd.map(tuple)\n",
    "    metrics = MulticlassMetrics(pl)\n",
    "    cm = metrics.confusionMatrix()\n",
    "\n",
    "    # \n",
    "    class_temp = predictions.select(\"label\").groupBy(\"label\")\\\n",
    "                            .count().sort('count', ascending=False).toPandas()\n",
    "    class_names = class_temp[\"label\"].values.tolist()\n",
    "    \n",
    "    \n",
    "    print(class_names)\n",
    "    cm = cm.toArray()\n",
    "    if verb:\n",
    "        print(cm)\n",
    "    # add by rows to compute the recall\n",
    "    sumaf = []\n",
    "    if verb:\n",
    "        print(\"Recall \")\n",
    "    for i in range(len(class_names)):\n",
    "        suma = sum(cm[i])\n",
    "        if verb:\n",
    "            print(i,':',round(cm[i][i]/suma,2))\n",
    "        sumaf.append(suma)\n",
    "              \n",
    "    # add by columns to compute the precision\n",
    "    sumac = []\n",
    "    sumad = []\n",
    "    if verb:\n",
    "        print(\"Precision \")\n",
    "    for i in range(len(class_names)):\n",
    "        suma = 0\n",
    "        for j in range(len(class_names)):\n",
    "              suma += cm[j][i]\n",
    "        sumac.append(suma)\n",
    "        if verb:\n",
    "            print(i,':',round(cm[i][i]/suma,2))\n",
    "        sumad.append(cm[i][i])\n",
    "    oa = np.sum(sumad)/sum(sumac)\n",
    "    ac = 0\n",
    "    for i in range(len(sumac)):\n",
    "        ac += sumac[i]*sumaf[i]\n",
    "    ac /= (sum(sumac)*sum(sumac))\n",
    "    #print(oa,ac)\n",
    "    kappa = (oa-ac)/(1-ac)\n",
    "    if verb:\n",
    "        # Instantiate metrics object\n",
    "        #precision = metrics.precision()\n",
    "        #recall = metrics.recall()\n",
    "        #f1Score = metrics.fMeasure()\n",
    "        print(\"***Global Statistics***\")\n",
    "        #print(\"Precision = %s\" % precision)\n",
    "        #print(\"Recall = %s\" % recall)\n",
    "        #print(\"F1 Score = %s\" % f1Score)\n",
    "        #print(\"Area under ROC = %s\" % metrics.areaUnderROC)        \n",
    "        print(\"kappa \",round(kappa,3))\n",
    "        acc = pl.filter(lambda x: x[0] == x[1]).count() / float(pl.count())\n",
    "        print(\"Model accuracy: %.3f%%\" % (acc * 100))\n",
    "    return cm,kappa\n",
    "\n",
    "def histogram(df,col,bins=20,color=\"red\"):\n",
    "    fig,ax = plt.subplots()\n",
    "    hist(ax, df.select([col]), bins = bins, color=[color])\n",
    "    plt.xlabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "f = urllib.request.urlretrieve(url,file ) # in case of error download manually and comment this line\n",
    "# load the dataframe\n",
    "df = load_file(file)\n",
    "print(f\"{df.count()} rows\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write here your code, remember that the final model must be in a varible `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### solution\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should return OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model\n",
    "    print(\"Ok!\")\n",
    "except NameError:\n",
    "    print(\"Error, the variable 'model' must exist!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "The final test. You can execute but no change this part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/RafaelCaballero/tdm/master/datos/filetrain.csv\"\n",
    "import urllib.request\n",
    "filetest = \"./test.csv\"\n",
    "f = urllib.request.urlretrieve(url,filetest ) # in case of error download manually and comment this line\n",
    "# load the rest file\n",
    "df_mytest = load_file(filetest)\n",
    "\n",
    "predictions = model.transform(df_mytest)\n",
    "cm,kappa = evaluate_predictions(predictions,verb=False)\n",
    "print(kappa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
